
In Principal Component Analysis the goal is to represent $N$-dimensional data using less then N numbers. This is done by finding $M$ orthogonal directions in which the data have the most variance and ignore the directions in where the data do not variance much. These $M$ principal directions form a lower-dimensional subspace and a $N$-dimensional data point can be represented by its projections onto these $M$ directions in the lower dimension subspace. The information about where the data is located in the remaining $N-M$ orthogonal directions are therefore lost. Since these do not have much variance, only a little information is lost.

\begin{figure}[hbtp]
\centering
\includegraphics[width=\linewidth]{var_explained}
\caption{The variance explained by $M$ Components.\label{fig:pca_var_explained}}
\end{figure}

\subsection{Percent Variance Explained by $M$ Components}

In Figure~\ref{fig:pca_var_explained} the variance explained by $1$ to $M$ principal components are shown where the explained percent for one component are shown as around 19 percent. Then the components needed to explain 60, 80, 90, 96, 97 and 100 percent are marked out on the x axis respective to the percent explained variance on the y axis.

\subsection{Principal Components Directions}

\subsection{Principal Component Projections}
