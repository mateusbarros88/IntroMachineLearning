\chapter{Classification}

\section{Problem}
We choose to solve the obvious classification problem for the MNIST data set, which is to classify the digit of each observation.

\section{Classification techniques}
% Apply at least three of the following methods:
% (Use cross-validation to select relevant parameters in an inner cross-validation
% loop and give in a table the performance results for the methods evaluated
% on the same cross-validation splits on the outer cross-validation loop, i.e. you
% should use two levels of cross-validation).

\subsection{Decision Trees}
Since we have a lot of attributes, which do not hold any significant meaning besides an seemingly arbitrary integer value, the decision tree solution is likely to not be very precise in determining the class. It will also be quite a bit tree since it has to value in each  of the different attributes, and determine what the meaning of that specific attribute might be in the classification.


\subsection{Logistic/Multinomial Regression}
\subsection{K-Nearest Neighbors (KNN)}
\subsection{Na√Øve Bayes}
\subsection{Artificial Neural Networks (ANN)}


\section{New data observation}
% For the models you are able to interpret explain how a new data observation
% is classified.
% (If you have multiple models fitted, (i.e., one for each cross-validation split)
% either focus on one of these fitted models or consider fitting one model for
% the optimal setting of the parameters estimated by cross-validation to all the
% data.)


\section{Performance comparison}
% Statistically compare the performance of the two best performing models (i.e.,
% use a paired t-test). Compare in addition if the performance of your models
% are better than simply predicting all outputs to be the largest class in the
% training data.